[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rancher-Ops",
    "section": "",
    "text": "Simply clone the repository at https://github.com/moonpiedumplings/rancher-ops\ncd rancher-ops/docker\ndocker-compose up. Or alternatively, docker compose up\nThen, rancher will be available at yourip:444. The ip you access rancher from is important, as it notes this and uses this ip to point things it controls, so don’t access it from localhost:444 or nonpublicvlan:444 or the like.\nI have set the default password to “password”, although you can change it in the docker-compose.yml file."
  },
  {
    "objectID": "index.html#networkmanager",
    "href": "index.html#networkmanager",
    "title": "Rancher-Ops",
    "section": "NetworkManager",
    "text": "NetworkManager\nIf your system is using networkmanager, like my virtual machines, were than, you need to add some things to make sure that networkmanager does not get in the way of the k3s/rke2 managed interfaces:\nhttps://docs.rke2.io/known_issues#networkmanager"
  },
  {
    "objectID": "index.html#calico-networking",
    "href": "index.html#calico-networking",
    "title": "Rancher-Ops",
    "section": "Calico Networking",
    "text": "Calico Networking\n\nInstalling K3s without flannel\nBecause vcluster needs calico networking, rather than the flannel that k3s offers by default, we need to make some changes to the installation process.\nIn the UI for the installation, select agent environment variables, and set them like so:\n\nHere is some code blocks that are copyable with one click, to make it easy:\nINSTALL_K3S_EXEC\n--flannel-backend=none --disable-network-policy\nThese wil set the necessary environment variable for the k3 installer that prevent it from installing flannel.\nI also recommend adding the environment variables listed in the setting up kubectl section of this document.\n\n\nInstalling Calico to a k3s cluster\nThe process is fairly simple, and the steps can be found on calico’s docs\nEnsure that you follow the manifest steps, the one with kubectl apply, because in my testing, using the operator steps will cause the cluster to perpetually hang on “waiting for cluster agent to connect” in rancher.\nThe current command for this step is:\nkubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml\nTwo things should be noted, if you are trying to automate this:\n\nYou can’t run kubectl in the same bash session as you run the install command, as the way it adds itself to path only takes effect after you reload or load a new bash shell\nThe kubernetes cluster takes a few seconds before it is ready to have kubectl ran against it. Commands ran earlier than that will fail.\n\nAnd now calico is installed and running. When everything is setup, rancher should be able to see the K3s cluster, and do management operations on it."
  },
  {
    "objectID": "index.html#setting-up-kubectl-to-work-without-sudo",
    "href": "index.html#setting-up-kubectl-to-work-without-sudo",
    "title": "Rancher-Ops",
    "section": "Setting up Kubectl to work without sudo",
    "text": "Setting up Kubectl to work without sudo\nWhile not strictly needed, using these agent environment variables:\n\n\nK3S_KUBECONFIG_MODE\n\n644\n\n\nWill enable kubecttl to run without needing sudo, if you are directly accessing one of the controller nodes on the cluster (k3s add kubectl and related tools to your path)"
  },
  {
    "objectID": "index.html#vcluster",
    "href": "index.html#vcluster",
    "title": "Rancher-Ops",
    "section": "Vcluster",
    "text": "Vcluster\nTo install vcluster on a system with kubectl configured and pointed at your specific cluster, simply follow the vcluster docs.\nTo create a vcluster, use vcluster create clustername --connect=false. This will prevent vcluster from creating and running a new proxy process which kubectl will try to connect to.\nTo list vclusters, vcluster list\nTo connect to a vcluster, use vcluster connect clusternmae -n &lt;optionalnamespace&gt; -- bash. This creates a new bash shell with the kubectl context switched to that vcluster. exit to exit."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Rancher-Ops Notes",
    "section": "",
    "text": "This document is mostly for personal usage, the steps I took to build the guides on the main page.\n\nKata Containers\nI found a guide to deploy Kata Containers on their github, using the kata-deploy tool.\nAccording to a related doc packaged install methods, kata-deploy is more for testing, and users are recommended to use the distro packages, as they have automatic updates. However, the only distro packages available are for Centos, and Fedora. 34.\nSome guides for ubuntu, like this one ask to you add a repository, but that repository is unmaintaned and has not been updated since 2019.\nThere is also an unmaintained snap package.\nAnother possibility is to use juju. Juju can add an externally managed kubernetes cluster, using the add-k8s command. (juju list-models will show where you can deploy to, and show-model gives more info.)\nThen, you can use the juju operator to deploy the kata charm\nThen, you should be able to upgrade the juju charm using juju refresh\nsudo snap install juju --classic\nexport PATH=/snap/bin:$PATH\njuju add-k8s mycluster\nBut I get an error:\nmoonpie@ubuntu:~$ juju add-k8s mycluster\nERROR cannot load ssh client keys: mkdir /home/moonpie/.local: permission denied\nmoonpie@ubuntu:~$ sudo juju add-k8s mycluster\n[sudo] password for moonpie: \nERROR stat .: permission denied\nmoonpie@ubuntu:~$ \nI don’t know why I get this error, but I am guessing it has something to do with snap sandboxing.\nOh, I missed some docs.\nmkdir -p ~/.local/share\nkubectl config view --raw | juju add-k8s mycluster --client\njuju add-model modelname\njuju deploy kata\nAnd it claims to have worked, but now my kubernetes command doesn’t work:\nmoonpie@ubuntu:~$ kubectl get all\nThe connection to the server 127.0.0.1:6443 was refused - did you specify the right host or port?\nThankfully, the kubernetes shell located within rancher still works (I had been using kubectl from the single ubuntu vm I provisioned to be a controller and worker).\nHowever, when I run the kata container test command, it doesn’t work:\nPods \"php-apache-kata-qemu-799d4cd788-\" is forbidden: pod rejected: RuntimeClass \"kata-qemu\" not found:Deployment does not have minimum availability.\nThis makes me think that kata containers weren’t actually installed.\nWell, I am first going to look into redownloading the kubeconfig file to the server, that way I can use both juju and kubectl on the same machine."
  }
]